{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import string\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_path = \"dataset/20_newsgroups\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a list of folder names to make valid pathnames later\n",
    "folders = [f for f in listdir(my_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a 2D list to store list of all files in different folders\n",
    "\n",
    "files = []\n",
    "for folder_name in folders:\n",
    "    folder_path = join(my_path, folder_name)\n",
    "    files.append([f for f in listdir(folder_path)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19997"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(len(files[i]) for i in range(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathname_list = []\n",
    "for fo in range(len(folders)):\n",
    "    for fi in files[fo]:\n",
    "        pathname_list.append(join(my_path, join(folders[fo], fi)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = []\n",
    "for folder_name in folders:\n",
    "    folder_path = join(my_path, folder_name)\n",
    "    num_of_files= len(listdir(folder_path))\n",
    "    for i in range(num_of_files):\n",
    "        Y.append(folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(pathname_list, Y, random_state=0, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Aditya\n",
      "[nltk_data]     Bhalsod\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords') \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "corpus = [] # corpus is collection of text(anything)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 1000): # 19998 are data but upper bound is exlude so we take 1000\n",
    "    review = re.sub('[^a-zA-Z]', ' ', X_train[i])\n",
    "    review = review.lower() # convert all captial letteres to lower\n",
    "    review = review.split() # it split the string into words\n",
    "    ps = PorterStemmer()\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]# stopwords is all the unnecessary words like the,place,is,that it removes. so ml algorithm can coreleate with best and suitable words\n",
    "    # we use set() because in large dataset like ariticles it hepls us to work fasetr. words('english') because here we use english comments.\n",
    "    # ps.stem(word) means it steamms the words like convert loved into love.\n",
    "    review = ' '.join(review) # we join seperate words in whole word but we the to not combine all 3 words we have to put space and then join the words.\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# define Tokenizer with Vocab Size\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words=15000)\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_matrix(X_train, mode='tfidf')\n",
    "X_test = tokenizer.texts_to_matrix(X_test, mode='tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "encoder = LabelBinarizer()\n",
    "#encoder.fit(train_tags)\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building ANN\n",
    "import keras\n",
    "from keras.models import Sequential # to initialize neural neiwork\n",
    "from keras.layers import Dense , Dropout # for densily connected neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0930 23:58:54.195729 15696 deprecation_wrapper.py:119] From C:\\Users\\Aditya Bhalsod\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# INittializing the ANN\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0930 23:58:54.353535 15696 deprecation_wrapper.py:119] From C:\\Users\\Aditya Bhalsod\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0930 23:58:54.358533 15696 deprecation_wrapper.py:119] From C:\\Users\\Aditya Bhalsod\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0930 23:58:54.400513 15696 deprecation_wrapper.py:119] From C:\\Users\\Aditya Bhalsod\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0930 23:58:54.421517 15696 deprecation.py:506] From C:\\Users\\Aditya Bhalsod\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# Adding the input layer and first hidden layer\n",
    "model.add(Dense(activation=\"relu\", input_dim=15000, units=512, kernel_initializer=\"uniform\"))\n",
    "model.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the second layer\n",
    "model.add(Dense(units = 512 , activation='relu' , kernel_initializer = \"uniform\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dropout(0.2))\n",
    "# Output layer\n",
    "model.add(Dense(units = 20 , activation='softmax' , kernel_initializer = \"uniform\")) # we use sigmoid because there is only one neuron but if we have ore then 2  output neuron then we have to use softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0930 23:58:54.973331 15696 deprecation_wrapper.py:119] From C:\\Users\\Aditya Bhalsod\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0930 23:58:55.024321 15696 deprecation_wrapper.py:119] From C:\\Users\\Aditya Bhalsod\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compile the ANN\n",
    "model.compile(optimizer='adam' , loss = 'categorical_crossentropy', metrics= ['accuracy']) # compile means we have to apply algorithm like gradient decent to find optimal set of weights in neuron because first we randomly initialize weight and now we have to apply the algorithm to find optimal set of weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0930 23:58:55.329326 15696 deprecation.py:323] From C:\\Users\\Aditya Bhalsod\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               7680512   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 20)                10260     \n",
      "=================================================================\n",
      "Total params: 7,953,428\n",
      "Trainable params: 7,953,428\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 13497 samples, validate on 1500 samples\n",
      "Epoch 1/20\n",
      "13497/13497 [==============================] - 87s 6ms/step - loss: 0.4509 - acc: 0.9296 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Epoch 2/20\n",
      "13497/13497 [==============================] - 60s 4ms/step - loss: 3.6457e-04 - acc: 1.0000 - val_loss: 6.7080e-04 - val_acc: 1.0000\n",
      "Epoch 3/20\n",
      "13497/13497 [==============================] - 54s 4ms/step - loss: 1.2490e-04 - acc: 1.0000 - val_loss: 5.3952e-04 - val_acc: 1.0000\n",
      "Epoch 4/20\n",
      "13497/13497 [==============================] - 49s 4ms/step - loss: 7.2319e-05 - acc: 1.0000 - val_loss: 4.6181e-04 - val_acc: 1.0000\n",
      "Epoch 5/20\n",
      "13497/13497 [==============================] - 51s 4ms/step - loss: 4.7593e-05 - acc: 1.0000 - val_loss: 4.0800e-04 - val_acc: 1.0000\n",
      "Epoch 6/20\n",
      "13497/13497 [==============================] - 49s 4ms/step - loss: 3.5369e-05 - acc: 1.0000 - val_loss: 3.6482e-04 - val_acc: 1.0000\n",
      "Epoch 7/20\n",
      "13497/13497 [==============================] - 49s 4ms/step - loss: 2.5650e-05 - acc: 1.0000 - val_loss: 3.3321e-04 - val_acc: 1.0000\n",
      "Epoch 8/20\n",
      "13497/13497 [==============================] - 47s 3ms/step - loss: 2.1131e-05 - acc: 1.0000 - val_loss: 3.0634e-04 - val_acc: 1.0000\n",
      "Epoch 9/20\n",
      "13497/13497 [==============================] - 48s 4ms/step - loss: 1.7270e-05 - acc: 1.0000 - val_loss: 2.8217e-04 - val_acc: 1.0000\n",
      "Epoch 10/20\n",
      "13497/13497 [==============================] - 51s 4ms/step - loss: 1.3765e-05 - acc: 1.0000 - val_loss: 2.6175e-04 - val_acc: 1.0000\n",
      "Epoch 11/20\n",
      "13497/13497 [==============================] - 52s 4ms/step - loss: 1.2171e-05 - acc: 1.0000 - val_loss: 2.4413e-04 - val_acc: 1.0000\n",
      "Epoch 12/20\n",
      "13497/13497 [==============================] - 49s 4ms/step - loss: 9.7054e-06 - acc: 1.0000 - val_loss: 2.2844e-04 - val_acc: 1.0000\n",
      "Epoch 13/20\n",
      "13497/13497 [==============================] - 50s 4ms/step - loss: 8.5340e-06 - acc: 1.0000 - val_loss: 2.1434e-04 - val_acc: 1.0000\n",
      "Epoch 14/20\n",
      "13497/13497 [==============================] - 54s 4ms/step - loss: 7.4577e-06 - acc: 1.0000 - val_loss: 2.0102e-04 - val_acc: 1.0000\n",
      "Epoch 15/20\n",
      "13497/13497 [==============================] - 49s 4ms/step - loss: 6.2758e-06 - acc: 1.0000 - val_loss: 1.8983e-04 - val_acc: 1.0000\n",
      "Epoch 16/20\n",
      "13497/13497 [==============================] - 47s 4ms/step - loss: 5.5330e-06 - acc: 1.0000 - val_loss: 1.7887e-04 - val_acc: 1.0000\n",
      "Epoch 17/20\n",
      "13497/13497 [==============================] - 48s 4ms/step - loss: 5.0158e-06 - acc: 1.0000 - val_loss: 1.6906e-04 - val_acc: 1.0000\n",
      "Epoch 18/20\n",
      "13497/13497 [==============================] - 47s 4ms/step - loss: 4.4542e-06 - acc: 1.0000 - val_loss: 1.6014e-04 - val_acc: 1.0000\n",
      "Epoch 19/20\n",
      "13497/13497 [==============================] - 50s 4ms/step - loss: 3.8562e-06 - acc: 1.0000 - val_loss: 1.5168e-04 - val_acc: 1.0000\n",
      "Epoch 20/20\n",
      "13497/13497 [==============================] - 49s 4ms/step - loss: 3.5317e-06 - acc: 1.0000 - val_loss: 1.4284e-04 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2d11a1ecb00>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary()\n",
    "\n",
    "# fitting the training set\n",
    "model.fit(X_train, y_train ,batch_size =80, epochs = 20,verbose=1,validation_split=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
